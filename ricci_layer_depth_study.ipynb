{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ricci Flow Layer Depth Study\n",
                "## DNN Training for Varying Layer Depths (3-30)\n",
                "\n",
                "This notebook trains DNNs with varying layer depths across three architectures (Narrow, Wide, Bottleneck) on MNIST 4 vs 9 binary classification.\n",
                "\n",
                "**Purpose:** Generate activations for Ricci curvature analysis to study how layer depth affects Ricci flow-like behavior.\n",
                "\n",
                "**Based on:** `training.py` and theoretical framework from \"Deep Learning as Ricci Flow\" (Baptista et al., 2024)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & GPU Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS\n",
                "# ============================================================================\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import time\n",
                "from tqdm.auto import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow/Keras imports\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense\n",
                "from tensorflow.keras.optimizers import RMSprop\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "from tensorflow.keras.datasets import mnist\n",
                "\n",
                "# ============================================================================\n",
                "# GPU DETECTION (for Kaggle/Colab)\n",
                "# ============================================================================\n",
                "print(\"=\" * 60)\n",
                "print(\"DEVICE DETECTION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Check for GPU\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    print(f\"✓ GPU(s) detected: {len(gpus)}\")\n",
                "    for gpu in gpus:\n",
                "        print(f\"  - {gpu.name}\")\n",
                "    # Enable memory growth to avoid OOM\n",
                "    try:\n",
                "        for gpu in gpus:\n",
                "            tf.config.experimental.set_memory_growth(gpu, True)\n",
                "        print(\"✓ GPU memory growth enabled\")\n",
                "    except RuntimeError as e:\n",
                "        print(f\"⚠ GPU memory growth setting failed: {e}\")\n",
                "else:\n",
                "    print(\"⚠ No GPU detected. Training will use CPU.\")\n",
                "    print(\"  Tip: Enable GPU in Kaggle/Colab settings for faster training.\")\n",
                "\n",
                "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "# Problem definition\n",
                "DIGIT_A = 4  # First class\n",
                "DIGIT_B = 9  # Second class (hard problem: 4 vs 9 are visually similar)\n",
                "\n",
                "# Layer depths to test (3 to 30)\n",
                "LAYER_DEPTHS = list(range(3, 31))  # [3, 4, 5, ..., 30]\n",
                "\n",
                "# Architecture configurations\n",
                "ARCHITECTURES = {\n",
                "    'narrow': {'width': 25, 'bottleneck': False},\n",
                "    'wide': {'width': 50, 'bottleneck': False},\n",
                "    'bottleneck': {'width': 50, 'bottleneck': True}  # 50 neurons first layer, 25 rest\n",
                "}\n",
                "\n",
                "# Training parameters (based on training.py)\n",
                "NUM_MODELS = 25  # Models per configuration for statistical robustness\n",
                "EPOCHS = 50\n",
                "BATCH_SIZE = 32\n",
                "VALIDATION_SPLIT = 0.2\n",
                "EARLY_STOP_ACCURACY = 0.99  # Stop when training accuracy hits 99%\n",
                "\n",
                "# Output directory\n",
                "OUTPUT_DIR = 'layer_depth_study_outputs'\n",
                "\n",
                "print(\"Configuration:\")\n",
                "print(f\"  Problem: MNIST {DIGIT_A} vs {DIGIT_B}\")\n",
                "print(f\"  Layer depths: {LAYER_DEPTHS[0]} to {LAYER_DEPTHS[-1]} ({len(LAYER_DEPTHS)} values)\")\n",
                "print(f\"  Architectures: {list(ARCHITECTURES.keys())}\")\n",
                "print(f\"  Models per config: {NUM_MODELS}\")\n",
                "print(f\"  Early stopping: {EARLY_STOP_ACCURACY*100}% training accuracy\")\n",
                "print(f\"  Total training runs: {len(ARCHITECTURES) * len(LAYER_DEPTHS) * NUM_MODELS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading (MNIST 4 vs 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA LOADING\n",
                "# ============================================================================\n",
                "\n",
                "def load_mnist_binary(digit_a, digit_b):\n",
                "    \"\"\"Load MNIST data filtered for binary classification.\"\"\"\n",
                "    # Load full MNIST\n",
                "    (x_train_full, y_train_full), (x_test_full, y_test_full) = mnist.load_data()\n",
                "    \n",
                "    # Filter for our two digits\n",
                "    train_mask = (y_train_full == digit_a) | (y_train_full == digit_b)\n",
                "    test_mask = (y_test_full == digit_a) | (y_test_full == digit_b)\n",
                "    \n",
                "    x_train = x_train_full[train_mask]\n",
                "    y_train = y_train_full[train_mask]\n",
                "    x_test = x_test_full[test_mask]\n",
                "    y_test = y_test_full[test_mask]\n",
                "    \n",
                "    # Flatten images (28x28 -> 784)\n",
                "    x_train = x_train.reshape(-1, 784).astype('float32')\n",
                "    x_test = x_test.reshape(-1, 784).astype('float32')\n",
                "    \n",
                "    # Normalize to [0, 1]\n",
                "    x_train = x_train / 255.0\n",
                "    x_test = x_test / 255.0\n",
                "    \n",
                "    # Convert labels to binary (digit_a -> 0, digit_b -> 1)\n",
                "    y_train = (y_train == digit_b).astype('int32')\n",
                "    y_test = (y_test == digit_b).astype('int32')\n",
                "    \n",
                "    return x_train, y_train, x_test, y_test\n",
                "\n",
                "# Load data\n",
                "print(\"Loading MNIST data...\")\n",
                "x_train, y_train, x_test, y_test = load_mnist_binary(DIGIT_A, DIGIT_B)\n",
                "\n",
                "print(f\"\\nDataset loaded:\")\n",
                "print(f\"  Training samples: {x_train.shape[0]} (Class 0: {np.sum(y_train==0)}, Class 1: {np.sum(y_train==1)})\")\n",
                "print(f\"  Test samples: {x_test.shape[0]} (Class 0: {np.sum(y_test==0)}, Class 1: {np.sum(y_test==1)})\")\n",
                "print(f\"  Feature dimension: {x_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Architecture Builders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ARCHITECTURE BUILDERS\n",
                "# ============================================================================\n",
                "\n",
                "def build_dnn(arch_name, arch_config, depth, input_dim):\n",
                "    \"\"\"\n",
                "    Build DNN model based on architecture configuration.\n",
                "    \n",
                "    Based on training.py structure:\n",
                "    - Narrow: 25 neurons per layer\n",
                "    - Wide: 50 neurons per layer\n",
                "    - Bottleneck: 50 neurons first layer, 25 for rest\n",
                "    \"\"\"\n",
                "    model = Sequential()\n",
                "    \n",
                "    width = arch_config['width']\n",
                "    is_bottleneck = arch_config['bottleneck']\n",
                "    \n",
                "    # First hidden layer\n",
                "    if is_bottleneck:\n",
                "        model.add(Dense(units=50, activation='relu', input_shape=(input_dim,)))\n",
                "    else:\n",
                "        model.add(Dense(units=width, activation='relu', input_shape=(input_dim,)))\n",
                "    \n",
                "    # Remaining hidden layers (depth - 1 more layers)\n",
                "    for _ in range(depth - 1):\n",
                "        if is_bottleneck:\n",
                "            model.add(Dense(units=25, activation='relu'))\n",
                "        else:\n",
                "            model.add(Dense(units=width, activation='relu'))\n",
                "    \n",
                "    # Output layer\n",
                "    model.add(Dense(units=1, activation='sigmoid'))\n",
                "    \n",
                "    # Compile (based on training.py)\n",
                "    model.compile(\n",
                "        loss='binary_crossentropy',\n",
                "        optimizer=RMSprop(),\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "# Test architecture building\n",
                "print(\"Testing architecture builders...\")\n",
                "for arch_name, arch_config in ARCHITECTURES.items():\n",
                "    test_model = build_dnn(arch_name, arch_config, depth=5, input_dim=784)\n",
                "    print(f\"  {arch_name}: {len(test_model.layers)} layers ({test_model.count_params()} params)\")\n",
                "print(\"✓ All architectures build successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Function with Early Stopping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# TRAINING FUNCTION\n",
                "# ============================================================================\n",
                "\n",
                "class AccuracyThresholdCallback(tf.keras.callbacks.Callback):\n",
                "    \"\"\"Stop training when training accuracy reaches threshold.\"\"\"\n",
                "    def __init__(self, threshold=0.99):\n",
                "        super().__init__()\n",
                "        self.threshold = threshold\n",
                "    \n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        if logs.get('accuracy') >= self.threshold:\n",
                "            self.model.stop_training = True\n",
                "\n",
                "\n",
                "def train_single_model(arch_name, arch_config, depth, x_train, y_train, x_test, y_test):\n",
                "    \"\"\"\n",
                "    Train a single DNN model and extract activations.\n",
                "    \n",
                "    Returns:\n",
                "        activations: list of numpy arrays (one per hidden layer)\n",
                "        accuracy: test accuracy\n",
                "    \"\"\"\n",
                "    # Build model\n",
                "    model = build_dnn(arch_name, arch_config, depth, input_dim=x_train.shape[1])\n",
                "    \n",
                "    # Early stopping callback (stop at 99% training accuracy)\n",
                "    early_stop = AccuracyThresholdCallback(threshold=EARLY_STOP_ACCURACY)\n",
                "    \n",
                "    # Train model (based on training.py)\n",
                "    model.fit(\n",
                "        x_train, y_train,\n",
                "        epochs=EPOCHS,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        validation_split=VALIDATION_SPLIT,\n",
                "        callbacks=[early_stop],\n",
                "        verbose=0\n",
                "    )\n",
                "    \n",
                "    # Evaluate on test set\n",
                "    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
                "    \n",
                "    # Extract activations from all hidden layers (exclude output layer)\n",
                "    # Based on training.py: model_predict[j] = activations\n",
                "    activations = []\n",
                "    current_input = x_test\n",
                "    for layer in model.layers[:-1]:  # Exclude output layer\n",
                "        current_output = layer(current_input)\n",
                "        activations.append(current_output.numpy())\n",
                "        current_input = current_output\n",
                "    \n",
                "    return activations, accuracy\n",
                "\n",
                "\n",
                "print(\"Training function defined.\")\n",
                "print(f\"  Early stopping: Training accuracy >= {EARLY_STOP_ACCURACY*100}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Output Directory Structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# OUTPUT STRUCTURE HELPER\n",
                "# ============================================================================\n",
                "\n",
                "def create_output_dirs():\n",
                "    \"\"\"\n",
                "    Create output directory structure:\n",
                "    \n",
                "    layer_depth_study_outputs/\n",
                "    ├── narrow/\n",
                "    │   ├── depth_3/\n",
                "    │   │   └── models_b25/\n",
                "    │   ├── depth_4/\n",
                "    │   └── ...\n",
                "    ├── wide/\n",
                "    │   └── ...\n",
                "    └── bottleneck/\n",
                "        └── ...\n",
                "    \"\"\"\n",
                "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "    \n",
                "    for arch_name in ARCHITECTURES.keys():\n",
                "        for depth in LAYER_DEPTHS:\n",
                "            path = os.path.join(OUTPUT_DIR, arch_name, f\"depth_{depth}\", f\"models_b{NUM_MODELS}\")\n",
                "            os.makedirs(path, exist_ok=True)\n",
                "    \n",
                "    print(f\"✓ Output directory structure created: {OUTPUT_DIR}/\")\n",
                "    print(f\"  Total folders: {len(ARCHITECTURES) * len(LAYER_DEPTHS)}\")\n",
                "\n",
                "\n",
                "def save_model_outputs(arch_name, depth, model_predict, accuracy_list, x_test, y_test):\n",
                "    \"\"\"\n",
                "    Save model outputs in format compatible with knn_fixed.py:\n",
                "    - model_predict.npy: object array of activation lists\n",
                "    - accuracy.npy: array of accuracy values\n",
                "    - x_test.csv: test features (headerless)\n",
                "    - y_test.csv: test labels (headerless)\n",
                "    \"\"\"\n",
                "    output_path = os.path.join(OUTPUT_DIR, arch_name, f\"depth_{depth}\", f\"models_b{NUM_MODELS}\")\n",
                "    \n",
                "    np.save(os.path.join(output_path, \"model_predict.npy\"), model_predict)\n",
                "    np.save(os.path.join(output_path, \"accuracy.npy\"), np.array(accuracy_list))\n",
                "    pd.DataFrame(x_test).to_csv(os.path.join(output_path, \"x_test.csv\"), index=False, header=None)\n",
                "    pd.DataFrame(y_test).to_csv(os.path.join(output_path, \"y_test.csv\"), index=False, header=None)\n",
                "\n",
                "\n",
                "# Create directory structure\n",
                "create_output_dirs()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Main Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# MAIN TRAINING LOOP\n",
                "# ============================================================================\n",
                "\n",
                "# Storage for summary CSV\n",
                "summary_results = []\n",
                "\n",
                "# Total configurations\n",
                "total_configs = len(ARCHITECTURES) * len(LAYER_DEPTHS)\n",
                "config_count = 0\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"TRAINING STARTED\")\n",
                "print(f\"Total configurations: {total_configs}\")\n",
                "print(f\"Models per configuration: {NUM_MODELS}\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "start_time_total = time.time()\n",
                "\n",
                "# Loop over architectures\n",
                "for arch_name, arch_config in ARCHITECTURES.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"ARCHITECTURE: {arch_name.upper()}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Loop over layer depths\n",
                "    for depth in tqdm(LAYER_DEPTHS, desc=f\"{arch_name} depths\"):\n",
                "        config_count += 1\n",
                "        \n",
                "        # Storage for this configuration\n",
                "        model_predict = np.empty(NUM_MODELS, dtype=object)\n",
                "        accuracy_list = []\n",
                "        \n",
                "        # Train NUM_MODELS models\n",
                "        for j in range(NUM_MODELS):\n",
                "            activations, accuracy = train_single_model(\n",
                "                arch_name, arch_config, depth,\n",
                "                x_train, y_train, x_test, y_test\n",
                "            )\n",
                "            model_predict[j] = activations\n",
                "            accuracy_list.append(accuracy)\n",
                "        \n",
                "        # Save outputs (compatible with knn_fixed.py)\n",
                "        save_model_outputs(arch_name, depth, model_predict, accuracy_list, x_test, y_test)\n",
                "        \n",
                "        # Store summary statistics\n",
                "        mean_acc = np.mean(accuracy_list)\n",
                "        std_acc = np.std(accuracy_list)\n",
                "        min_acc = np.min(accuracy_list)\n",
                "        max_acc = np.max(accuracy_list)\n",
                "        \n",
                "        summary_results.append({\n",
                "            'architecture': arch_name,\n",
                "            'depth': depth,\n",
                "            'num_models': NUM_MODELS,\n",
                "            'mean_accuracy': mean_acc,\n",
                "            'std_accuracy': std_acc,\n",
                "            'min_accuracy': min_acc,\n",
                "            'max_accuracy': max_acc\n",
                "        })\n",
                "        \n",
                "        # Progress update every 7 depths\n",
                "        if depth % 7 == 0:\n",
                "            tqdm.write(f\"  depth={depth}: mean_acc={mean_acc:.4f} ± {std_acc:.4f}\")\n",
                "\n",
                "total_time = time.time() - start_time_total\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"TRAINING COMPLETE!\")\n",
                "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
                "print(f\"{'='*80}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Summary CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SAVE SUMMARY CSV\n",
                "# ============================================================================\n",
                "\n",
                "# Create DataFrame\n",
                "summary_df = pd.DataFrame(summary_results)\n",
                "\n",
                "# Save to CSV\n",
                "summary_csv_path = os.path.join(OUTPUT_DIR, 'training_summary.csv')\n",
                "summary_df.to_csv(summary_csv_path, index=False)\n",
                "\n",
                "print(f\"Summary saved to: {summary_csv_path}\")\n",
                "print(f\"\\nSummary statistics:\")\n",
                "print(summary_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Quick Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# QUICK VISUALIZATION\n",
                "# ============================================================================\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
                "\n",
                "for arch_name in ARCHITECTURES.keys():\n",
                "    arch_data = summary_df[summary_df['architecture'] == arch_name]\n",
                "    ax.errorbar(\n",
                "        arch_data['depth'], \n",
                "        arch_data['mean_accuracy'],\n",
                "        yerr=arch_data['std_accuracy'],\n",
                "        label=arch_name,\n",
                "        marker='o',\n",
                "        capsize=3\n",
                "    )\n",
                "\n",
                "ax.set_xlabel('Number of Hidden Layers (Depth)', fontsize=12)\n",
                "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
                "ax.set_title(f'MNIST {DIGIT_A} vs {DIGIT_B}: Accuracy vs Layer Depth', fontsize=14)\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.set_xlim([2.5, 30.5])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(OUTPUT_DIR, 'accuracy_vs_depth.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Visualization saved to: {OUTPUT_DIR}/accuracy_vs_depth.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Verification & Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# VERIFICATION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"VERIFICATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Check a sample output\n",
                "sample_path = os.path.join(OUTPUT_DIR, 'narrow', 'depth_5', f'models_b{NUM_MODELS}')\n",
                "print(f\"\\nChecking sample output: {sample_path}\")\n",
                "\n",
                "# Load and verify\n",
                "sample_acc = np.load(os.path.join(sample_path, 'accuracy.npy'))\n",
                "sample_model = np.load(os.path.join(sample_path, 'model_predict.npy'), allow_pickle=True)\n",
                "\n",
                "print(f\"  accuracy.npy shape: {sample_acc.shape}\")\n",
                "print(f\"  accuracy range: [{sample_acc.min():.4f}, {sample_acc.max():.4f}]\")\n",
                "print(f\"  model_predict.npy: {len(sample_model)} models\")\n",
                "print(f\"  Activations per model: {len(sample_model[0])} layers\")\n",
                "print(f\"  Activation shapes: {[a.shape for a in sample_model[0]]}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"NEXT STEPS\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\"\"\n",
                "The outputs are now ready for Ricci curvature analysis using knn_fixed.py.\n",
                "\n",
                "For each architecture/depth combination:\n",
                "  1. Load model_predict.npy and x_test.csv\n",
                "  2. Build kNN graphs on activations\n",
                "  3. Compute Forman-Ricci curvature\n",
                "  4. Correlate with accuracy from training_summary.csv\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}